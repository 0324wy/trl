{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2 with value head\n",
    "> A GPT2 model with a value head built on the transformer library by huggingface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why a value head?\n",
    "Optimisation through PPO requires estimates on the current states value. The value can be estimated by adding a second head to the GPT2 model which outputs a scalar for each output token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detach head\n",
    "I experimented with detaching the head from the body when optimizing the model. This means that only the head is trained and the gradients are not passed through the body. Although I did not use it in the end it is still possible to detach the head by calling `model.detach_head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Model, GPT2PreTrainedModel\n",
    "from torch import nn\n",
    "from torch.nn import Identity\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "class ValueHead(nn.Module):\n",
    "    \"\"\"The ValueHead class implements a head for GPT2 that returns a scalar for each output token.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.detach_head = False\n",
    "        self.summary_type = config.summary_type if hasattr(config, \"summary_type\") else \"last\"\n",
    "        if self.summary_type == \"attn\":\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.summary = Identity()\n",
    "        if hasattr(config, \"summary_use_proj\") and config.summary_use_proj:\n",
    "            if hasattr(config, \"summary_proj_to_labels\") and config.summary_proj_to_labels and config.num_labels > 0:\n",
    "                num_classes = config.num_labels\n",
    "            else:\n",
    "                num_classes = config.hidden_size\n",
    "            self.summary = nn.Linear(config.hidden_size, num_classes)\n",
    "\n",
    "        self.activation = Identity()\n",
    "        if hasattr(config, \"summary_activation\") and config.summary_activation == \"tanh\":\n",
    "            self.activation = nn.Tanh()\n",
    "\n",
    "        self.first_dropout = Identity()\n",
    "        if hasattr(config, \"summary_first_dropout\") and config.summary_first_dropout > 0:\n",
    "            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n",
    "\n",
    "        self.last_dropout = Identity()\n",
    "        if hasattr(config, \"summary_last_dropout\") and config.summary_last_dropout > 0:\n",
    "            self.last_dropout = nn.Dropout(config.summary_last_dropout)\n",
    "            \n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, hidden_states, cls_index=None):\n",
    "        if self.detach_head:\n",
    "            output = hidden_states.detach()\n",
    "        else:\n",
    "            output = hidden_states\n",
    "        output = self.first_dropout(output)\n",
    "        output = self.summary(output)\n",
    "        output = self.activation(output)\n",
    "        output = self.last_dropout(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "class GPT2HeadWithValueModel(GPT2PreTrainedModel):\n",
    "    \"\"\"The GPT2HeadWithValueModel class implements a GPT2 language model with a secondary, scalar head.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        config.num_labels = 1\n",
    "        self.transformer = GPT2Model(config)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.v_head = ValueHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def detach_value_head(self):\n",
    "        self.v_head.detach_head = True\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        past=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        mc_token_ids=None,\n",
    "        lm_labels=None,\n",
    "        mc_labels=None,\n",
    "    ):\n",
    "       \n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            past=past,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        hidden_states = transformer_outputs[0]\n",
    "\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "        value = self.v_head(hidden_states).squeeze(-1)\n",
    "\n",
    "        outputs = (lm_logits,) + transformer_outputs[1:] + (value,)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a pre-trained language model\n",
    "Loading a pretrained language model works like loading it with a model from the `transformer` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2HeadWithValueModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_txt = \"I liked the movie Transformers!\" + tokenizer.eos_token\n",
    "input_ids = tokenizer.encode(input_txt, add_special_tokens=True, return_tensors=\"pt\")\n",
    "logits, transformer_outputs, values = model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We input a batch of `1` with `7` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logits tensor is of shape `[batch_size, num_input_tokens, vocab_size]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 50257])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value tensor is of shape `[batch_size, num_input_tokens]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can greedy decode the next token predictions from the logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ids = torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I --> .\n",
      " liked -->  the\n",
      " the -->  idea\n",
      " movie --> ,\n",
      " Transformers --> ,\n",
      "! -->  I\n",
      "<|endoftext|> --> The\n"
     ]
    }
   ],
   "source": [
    "for i in range(input_ids.shape[1]):\n",
    "    current_id = tokenizer.decode(input_ids[:, i])\n",
    "    next_id = tokenizer.decode(pred_ids[:, i])\n",
    "    print(current_id, '-->', next_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Respond to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "def respond(model,\n",
    "            query,\n",
    "            text_length=50,\n",
    "            temperature=1.0,\n",
    "            k=0,\n",
    "            p=1.,\n",
    "            repetition_penalty=1.0,\n",
    "            pad_token_id=None):\n",
    "    \n",
    "    output_sequences = model.generate(\n",
    "        input_ids=query,\n",
    "        max_length=text_length,\n",
    "        min_length=text_length,\n",
    "        temperature=temperature,\n",
    "        top_k=k,\n",
    "        top_p=p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        do_sample=True,\n",
    "        pad_token_id=pad_token_id,\n",
    "    )\n",
    "\n",
    "    generated_sequence = output_sequences[0]\n",
    "    generated_sequence = generated_sequence[query.shape[1]:]\n",
    "\n",
    "    return generated_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the computation to the GPU if possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "model.to(device)\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_txt=\"My favourite movie is\"\n",
    "query = tokenizer.encode(query_txt, return_tensors=\"pt\")\n",
    "query = query.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " not Spike's Animal Crossing and love him not A Link to the Past.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = respond(model, query, text_length=20, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batched response to queries\n",
    "To speed up computations it helps to process queries in a batched fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "def respond_to_batch(model,\n",
    "            queries,\n",
    "            text_length=50,\n",
    "            temperature=1.0,\n",
    "            k=0,\n",
    "            p=1.,\n",
    "            repetition_penalty=1.0,\n",
    "            pad_token_id=None):\n",
    "    \n",
    "    output_sequences = model.generate(\n",
    "        input_ids=queries,\n",
    "        max_length=text_length,\n",
    "        min_length=text_length,\n",
    "        temperature=temperature,\n",
    "        top_k=k,\n",
    "        top_p=p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        do_sample=True,\n",
    "        pad_token_id=pad_token_id,\n",
    "    )\n",
    "\n",
    "    generated_sequence = output_sequences[: ,queries.shape[1]:]\n",
    "\n",
    "    return generated_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the model respond to two queries in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([1, 5]), torch.Size([1, 5])]\n"
     ]
    }
   ],
   "source": [
    "query_txt_1 = \"My most favourite movie is\"\n",
    "query_txt_2 = \"My least favourite movie is\"\n",
    "queries_txt = [query_txt_1, query_txt_2]\n",
    "\n",
    "queries = [tokenizer.encode(query_txt, return_tensors=\"pt\") for query_txt in queries_txt]\n",
    "print([q.shape for q in queries])\n",
    "queries = torch.cat(queries)\n",
    "\n",
    "responses = respond_to_batch(model, queries, text_length=10, pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This only works because both queries have the same number of tokens. If that is not the case one must pad the tensors before stacking them in `torch.cat(queries)`.\n",
    "\n",
    "Then we can decode the responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My most favourite movie is the brilliant adaptation of the\n",
      "My least favourite movie is The Birth of a Nation\n"
     ]
    }
   ],
   "source": [
    "for i in range(responses.shape[0]):\n",
    "    response_txt = tokenizer.decode(responses[i])\n",
    "    query_txt = queries_txt[i]\n",
    "    print(query_txt + response_txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
